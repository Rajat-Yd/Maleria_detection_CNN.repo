{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwuUxWhrZX6O",
        "outputId": "c4ec5168-6578-4062-84f7-a42f166f0592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plots\n",
        "import sklearn### machine learning library\n",
        "import cv2## image processing\n",
        "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
        "import seaborn as sns### visualizations\n",
        "import datetime\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, InputLayer, BatchNormalization, Input, Dropout, RandomFlip, RandomRotation, Resizing, Rescaling\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, FalsePositives, FalseNegatives, TruePositives, TrueNegatives, Precision, Recall, AUC, binary_accuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers  import L2, L1\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from google.colab import drive"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "oi_Yb1RkZWFB",
        "outputId": "b1828cbc-4df4-42cc-a467-fe8fedcb6a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'keras._tf_keras.keras' has no attribute '__internal__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6c82ca0d97b2>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# `python/__init__.py` as necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubstrates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"TensorFlow Probability alternative substrates.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_loader\u001b[0m  \u001b[0;31m# pylint: disable=g-direct-tensorflow-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;31m# Non-lazy load of packages that register with tensorflow or keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpkg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_maybe_nonlazy_load\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forces loading the package from its lazy loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m__dir__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_first_access\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbayesopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbijectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/experimental/bayesopt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"TensorFlow Probability experimental Bayesopt package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macquisition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/experimental/bayesopt/acquisition/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition_function\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAcquisitionFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition_function\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMCMCReducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_improvement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianProcessExpectedImprovement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_improvement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelExpectedImprovement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_improvement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStudentTProcessExpectedImprovement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/experimental/bayesopt/acquisition/expected_improvement.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstudent_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macquisition_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpareto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPareto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_cnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPixelCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplackett_luce\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPlackettLuce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoisson\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoisson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/distributions/pixel_cnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreparameterization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorshape_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweight_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_variational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseReparameterization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_variational_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseVariational\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalMixtureOfOneHotCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributionLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndependentBernoulli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/distribution_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_symbolic_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TensorCoercible\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras._tf_keras.keras' has no attribute '__internal__'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFJwm1uxmRez"
      },
      "source": [
        "## Wandb Install, Login, Initialization and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SGRTsq6Nqzrm",
        "outputId": "b431d52f-09da-44f2-e630-5093aea605e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy2mEFDzpKwL",
        "outputId": "c2ddf590-87fd-428d-f308-48839220704c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.3-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 27.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 64.4 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 8.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 63.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 57.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 57.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 62.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 56.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=2e9b4ce856f8fd3e8af4d8092d480ae0e5fb73e6183da07d38081b80ba4b350c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7M99wtkhAT9"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-sLU81XmSBx",
        "outputId": "5addff8c-aec5-4203-8b58-339fd257cca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn6EL3iOtSbq"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\")\n",
        "\n",
        "###wandb.tensorboard.patch(root_logdir=\"./logs\")\n",
        "#wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\", sync_tensorboard=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKjsIPdavgSE"
      },
      "outputs": [],
      "source": [
        "wandb.config = {\n",
        "  \"LEARNING_RATE\": 0.001,\n",
        "  \"N_EPOCHS\": 5,\n",
        "  \"BATCH_SIZE\": 128,\n",
        "  \"DROPOUT_RATE\": 0.0,\n",
        "  \"IM_SIZE\": 224,\n",
        "  \"REGULARIZATION_RATE\": 0.0,\n",
        "  \"N_FILTERS\": 6,\n",
        "  \"KERNEL_SIZE\": 3,\n",
        "  \"N_STRIDES\": 1,\n",
        "  \"POOL_SIZE\": 2,\n",
        "  \"N_DENSE_1\": 100,\n",
        "  \"N_DENSE_2\": 10,\n",
        "}\n",
        "CONFIGURATION = wandb.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrGMGJ7Nb3Bh"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK7_p02_b6_m"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "899Bnj-olJGx"
      },
      "outputs": [],
      "source": [
        "dataset, dataset_info = tfds.load('malaria', with_info=True,\n",
        "                                  as_supervised=True,\n",
        "                                  shuffle_files = True,\n",
        "                                  split=['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XixoxMwuKG_-"
      },
      "outputs": [],
      "source": [
        "for data in dataset[0].take(1):\n",
        "  print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syf8XttCCWzX"
      },
      "outputs": [],
      "source": [
        "for i in d.take(1):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPBOQ3B_cIpx"
      },
      "outputs": [],
      "source": [
        "#dataset_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF9ACbZwHhm3"
      },
      "outputs": [],
      "source": [
        "def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n",
        "  DATASET_SIZE = len(dataset)\n",
        "\n",
        "  train_dataset = dataset.take(int(TRAIN_RATIO*DATASET_SIZE))\n",
        "\n",
        "  val_test_dataset = dataset.skip(int(TRAIN_RATIO*DATASET_SIZE))\n",
        "  val_dataset = val_test_dataset.take(int(VAL_RATIO*DATASET_SIZE))\n",
        "\n",
        "  test_dataset = val_test_dataset.skip(int(VAL_RATIO*DATASET_SIZE))\n",
        "  return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUiEq8Ew9AwN"
      },
      "outputs": [],
      "source": [
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO )\n",
        "#print(list(train_dataset.take(1).as_numpy_iterator()),\n",
        " #     list(val_dataset.take(1).as_numpy_iterator()), list(test_dataset.take(1).as_numpy_iterator()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5otp5EUU2cy"
      },
      "outputs": [],
      "source": [
        "train_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1QWh3KjbsuJ"
      },
      "source": [
        "## Dataset Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l7Lre1oUzx2"
      },
      "outputs": [],
      "source": [
        "for i, (image, label) in enumerate(train_dataset.take(16)):\n",
        "  ax = plt.subplot(4, 4, i + 1)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  plt.title(dataset_info.features['label'].int2str(label))\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8btu7qeen1hX"
      },
      "outputs": [],
      "source": [
        "# for i, (image, label) in enumerate(train_dataset.take(2)):\n",
        "#   plt.subplot(1, 4, 2*i + 1)\n",
        "#   plt.imshow(image)\n",
        "\n",
        "#   plt.subplot(1, 4, 2*i + 2)\n",
        "#   plt.imshow(tf.image.adjust_saturation(image, 0.3))\n",
        "\n",
        "\n",
        "#   plt.title(dataset_info.features['label'].int2str(label))\n",
        "#   plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT8DiQxbVMz7"
      },
      "outputs": [],
      "source": [
        "dataset_info.features['label'].int2str(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Gqdfs9kjv3"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BUZMRuhV-HL"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKFOnzZQ7Fya"
      },
      "outputs": [],
      "source": [
        "def visualize(original, augmented):\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(original)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(augmented)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMWtOY6e7OH-"
      },
      "outputs": [],
      "source": [
        "original_image, label = next(iter(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aKifNjs7OSe"
      },
      "outputs": [],
      "source": [
        "augmented_image = tf.image.adjust_saturation(original_image, saturation_factor = 0.3)#central_crop(original_image, 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo80zT197OT-"
      },
      "outputs": [],
      "source": [
        "visualize(original_image, augmented_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIXxlS6LVKmU"
      },
      "outputs": [],
      "source": [
        "IM_SIZE = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BKlhCPaR61p"
      },
      "outputs": [],
      "source": [
        "original_image, label = next(iter(train_dataset))\n",
        "@tf.function\n",
        "def resize_rescale(image, label):\n",
        "  #print(\"I was here\")\n",
        "  #tf.print(\"I was here\")\n",
        "  return tf.image.resize(image, (IM_SIZE, IM_SIZE))/255.0, label\n",
        "\n",
        "_, _ = resize_rescale(original_image, label)\n",
        "_, _ = resize_rescale(original_image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7_OVJEtZQWV"
      },
      "outputs": [],
      "source": [
        "#tf.config.run_functions_eagerly(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dyO-k9mMYTc"
      },
      "outputs": [],
      "source": [
        "### tf.keras.layer resizing and rescaling\n",
        "resize_rescale_layers = tf.keras.Sequential([\n",
        "       Resizing(IM_SIZE, IM_SIZE),\n",
        "       Rescaling(1./255),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBXnNGAcVgXa"
      },
      "outputs": [],
      "source": [
        "### tf.image augment\n",
        "@tf.function\n",
        "def augment(image, label):\n",
        "  image, label = resize_rescale(image, label)\n",
        "\n",
        "  image = tf.image.rot90(image)\n",
        "  #image = tf.image.adjust_saturation(image, saturation_factor = 0.3)\n",
        "  image = tf.image.flip_left_right(image)\n",
        "\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ck5ElZT53qd"
      },
      "outputs": [],
      "source": [
        "class RotNinety(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  @tf.function\n",
        "  def call(self, image):\n",
        "    return tf.image.rot90(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii6zZKmX9cur"
      },
      "outputs": [],
      "source": [
        "### tf.keras.layer augment\n",
        "augment_layers = tf.keras.Sequential([\n",
        "       RandomRotation(factor = (0.25, 0.2501),),\n",
        "       RandomFlip(mode='horizontal',),\n",
        "       RandomContrast(factor=0.1),\n",
        "\n",
        "])\n",
        "\n",
        "@tf.function\n",
        "def augment_layer(image, label):\n",
        "  return augment_layers(resize_rescale_layers(image), training = True), label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_RZU3vxG2Aj"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaDqyvwZ7D6a"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLkx-Ct8m8Ak"
      },
      "outputs": [],
      "source": [
        "test_dataset = test_dataset.map(resize_rescale, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "#train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XbLitL4m9DA"
      },
      "outputs": [],
      "source": [
        "# for image,label in train_dataset.take(1):\n",
        "#   print(image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5ydz6dhm9Fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .map(augment_layer, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF4UBrf5eNmQ"
      },
      "outputs": [],
      "source": [
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .shuffle(buffer_size = 32)\n",
        "    .map(resize_rescale, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOK_U7-neNuy"
      },
      "outputs": [],
      "source": [
        "val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDlvxwC1m9Hl"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im-Eo6tvHgmM"
      },
      "source": [
        "### MIxup Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI_b_cBzJeRv"
      },
      "outputs": [],
      "source": [
        "train_dataset_1 = train_dataset.shuffle(buffer_size = 4096, ).map(resize_rescale, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "train_dataset_2 = train_dataset.shuffle(buffer_size = 4096, ).map(resize_rescale, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "\n",
        "mixed_dataset = tf.data.Dataset.zip((train_dataset_1, train_dataset_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kaku3IHHkVN"
      },
      "outputs": [],
      "source": [
        "def mixup(train_dataset_1, train_dataset_2):\n",
        "  (image_1,label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n",
        "\n",
        "  lamda = tfp.distributions.Beta(0.2,0.2)\n",
        "  lamda = lamda.sample(1)[0]\n",
        "\n",
        "  image = lamda*image_1 + (1-lamda)*image_2\n",
        "  label = lamda*tf.cast(label_1, dtype = tf.float32) + (1-lamda)*tf.cast(label_2, dtype = tf.float32)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uwj65QhL5ST"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = (\n",
        "    mixed_dataset\n",
        "    .shuffle(buffer_size = 4096, reshuffle_each_iteration = True)\n",
        "    .map(mixup, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    #map(cutmix, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKYfVCHsMV41"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IylKvmx5LUCR"
      },
      "source": [
        "### CutMix Data *Augmentation*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4mfRVpjVKkz"
      },
      "outputs": [],
      "source": [
        "def box(lamda):\n",
        "\n",
        "  r_x = tf.cast(tfp.distributions.Uniform(0, IM_SIZE).sample(1)[0], dtype = tf.int32)\n",
        "  r_y = tf.cast(tfp.distributions.Uniform(0, IM_SIZE).sample(1)[0], dtype = tf.int32)\n",
        "\n",
        "  r_w = tf.cast(IM_SIZE*tf.math.sqrt(1-lamda), dtype = tf.int32)\n",
        "  r_h = tf.cast(IM_SIZE*tf.math.sqrt(1-lamda), dtype = tf.int32)\n",
        "\n",
        "  r_x = tf.clip_by_value(r_x - r_w//2, 0, IM_SIZE)\n",
        "  r_y = tf.clip_by_value(r_y - r_h//2, 0, IM_SIZE)\n",
        "\n",
        "  x_b_r = tf.clip_by_value(r_x + r_w//2, 0, IM_SIZE)\n",
        "  y_b_r = tf.clip_by_value(r_y + r_h//2, 0, IM_SIZE)\n",
        "\n",
        "  r_w = x_b_r - r_x\n",
        "  if(r_w == 0):\n",
        "    r_w  = 1\n",
        "\n",
        "  r_h = y_b_r - r_y\n",
        "  if(r_h == 0):\n",
        "    r_h = 1\n",
        "\n",
        "  return r_y, r_x, r_h, r_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDH9vwpMLWBd"
      },
      "outputs": [],
      "source": [
        "def cutmix(train_dataset_1, train_dataset_2):\n",
        "  (image_1,label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n",
        "\n",
        "  lamda = tfp.distributions.Beta(0.2,0.2)\n",
        "  lamda = lamda.sample(1)[0]\n",
        "\n",
        "  r_y, r_x, r_h, r_w = box(lamda)\n",
        "  crop_2 = tf.image.crop_to_bounding_box(image_2, r_y, r_x, r_h, r_w)\n",
        "  pad_2 = tf.image.pad_to_bounding_box(crop_2, r_y, r_x, IM_SIZE, IM_SIZE)\n",
        "\n",
        "  crop_1 = tf.image.crop_to_bounding_box(image_1, r_y, r_x, r_h, r_w)\n",
        "  pad_1 = tf.image.pad_to_bounding_box(crop_1, r_y, r_x, IM_SIZE, IM_SIZE)\n",
        "\n",
        "  image = image_1 - pad_1 + pad_2\n",
        "\n",
        "  lamda = tf.cast(1- (r_w*r_h)/(IM_SIZE*IM_SIZE), dtype = tf.float32)\n",
        "  label = lamda*tf.cast(label_1, dtype = tf.float32) + (1-lamda)*tf.cast(label_2, dtype = tf.float32)\n",
        "\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOKOaUrPwydO"
      },
      "outputs": [],
      "source": [
        "original_image, label = next(iter(train_dataset))\n",
        "print(label)\n",
        "plt.imshow(original_image[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_SDW1m61zA3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTW0fKw313gt"
      },
      "source": [
        "### Albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFgRO86M87ME"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWO057_PshWr"
      },
      "outputs": [],
      "source": [
        "transforms = A.Compose(\n",
        "    [\n",
        "      A.Resize(IM_SIZE, IM_SIZE),\n",
        "\n",
        "      A.OneOf([A.HorizontalFlip(),\n",
        "                A.VerticalFlip(),], p = 0.3),\n",
        "\n",
        "      A.RandomRotate90(),\n",
        "      #A.RandomGridShuffle(grid=(3, 3), always_apply=False, p=0.5),\n",
        "      A.RandomBrightnessContrast(brightness_limit=0.2,\n",
        "                                contrast_limit=0.2,\n",
        "                                always_apply=False, p=0.5),\n",
        "      #A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5),\n",
        "      A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=False, p=0.5),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlv9wdoRshZq"
      },
      "outputs": [],
      "source": [
        "def aug_albument(image):\n",
        "  data = {\"image\":image}\n",
        "  image = transforms(**data)\n",
        "  image = image[\"image\"]\n",
        "  image = tf.cast(image/255., tf.float32)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99IrNqzqEx9R"
      },
      "outputs": [],
      "source": [
        "def process_data(image, label):\n",
        "    aug_img = tf.numpy_function(func=aug_albument, inp=[image], Tout=tf.float32)\n",
        "    return aug_img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR1XQju8ELjx"
      },
      "outputs": [],
      "source": [
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .map(process_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyDZGZWPELm3"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAh74s-oELp_"
      },
      "outputs": [],
      "source": [
        "im, _ = next(iter(train_dataset))\n",
        "plt.imshow(im[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAC1aaCGELuJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "for i in range(1,32):\n",
        "  plt.subplot(8,4,i)\n",
        "  plt.imshow(im[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeVM_g7JsaQr"
      },
      "source": [
        "### Repeating the dataset (x5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP8axz4dzc3d"
      },
      "outputs": [],
      "source": [
        "def augment_1(image, label):\n",
        "  image, label = resize_rescale(image, label)\n",
        "\n",
        "  image = tf.image.random_brightness(image, 0.2)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZE2UFX9zgI5"
      },
      "outputs": [],
      "source": [
        "def augment_2(image, label):\n",
        "  image, label = resize_rescale(image, label)\n",
        "\n",
        "  image = tf.image.random_flip_up_down(image)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMnehRtxzmRA"
      },
      "outputs": [],
      "source": [
        "def augment_3(image, label):\n",
        "  image, label = resize_rescale(image, label)\n",
        "\n",
        "  image = tf.image.flip_left_right(image)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cq-vZyzzr1K"
      },
      "outputs": [],
      "source": [
        "def augment_4(image, label):\n",
        "  image, label = resize_rescale(image, label)\n",
        "\n",
        "  image = tf.image.rot90(image)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xz176pKqEp0c"
      },
      "outputs": [],
      "source": [
        "def augment_5(image, label):\n",
        "  image, label = resize_rescale(image, label)\n",
        "\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LidXL2P7tuAt"
      },
      "outputs": [],
      "source": [
        "train_dataset_1 = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .map(augment_1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBFQt8Q58QCU"
      },
      "outputs": [],
      "source": [
        "train_dataset_2 = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .map(augment_2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsTYAvRTEwvr"
      },
      "outputs": [],
      "source": [
        "train_dataset_3 = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .map(augment_3)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kUetjtmEw25"
      },
      "outputs": [],
      "source": [
        "train_dataset_4 = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .map(augment_4)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2LrVC7SEw7f"
      },
      "outputs": [],
      "source": [
        "train_dataset_5 = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .map(augment_5)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yT3s-zBuS5l"
      },
      "outputs": [],
      "source": [
        "full_dataset = train_dataset_1.concatenate(train_dataset_2).concatenate(train_dataset_3).concatenate(train_dataset_4).concatenate(train_dataset_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZti4FnyzTce"
      },
      "outputs": [],
      "source": [
        "full_dataset = (\n",
        "    full_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdIY_Vgv0DLt"
      },
      "outputs": [],
      "source": [
        "full_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn6iFj4SX3Sd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSX0Fu-vYFAk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM5OVNH9bHaW"
      },
      "source": [
        "## WandB Dataset Versioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsB9YnCuPITj"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXo1OjaOPMNj"
      },
      "outputs": [],
      "source": [
        "dataset, dataset_info = tfds.load('malaria', with_info=True, as_supervised=True, shuffle_files = True, split=['train'])\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQCKKSWI6HX5"
      },
      "outputs": [],
      "source": [
        "k = 0\n",
        "for data in dataset[0]:\n",
        "\n",
        "  with open('dataset/malaria_dataset_'+str(k) + '.npz', mode = 'wb') as file:\n",
        "      np.savez(file, data)\n",
        "  k += 1\n",
        "  if(k%1000 == 0):\n",
        "    print(k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpFw5pGy57Tj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWKy1DX9b5Sm"
      },
      "outputs": [],
      "source": [
        "def load_original_data():\n",
        "  with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "\n",
        "    original_data = wandb.Artifact(\n",
        "        name = \"new_dataset\",\n",
        "        type=\"raw_data\",\n",
        "        description = \"The Malaria dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells from the thin blood smear slide images of segmented cells.\",\n",
        "        metadata = {\"source\": \"TFDS\",\n",
        "                    \"homepage\": \"https://lhncbc.nlm.nih.gov/publication/pub9932\",\n",
        "                    \"source_code\": \"tfds.image_classification.Malaria\",\n",
        "                    \"version\": \"1.0.0\",\n",
        "                    \"download_size\": \"337.08 MiB\",\n",
        "                    }\n",
        "    )\n",
        "\n",
        "    original_data.add_dir('dataset/')\n",
        "\n",
        "    run.log_artifact(original_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k4w6y8-uKAL"
      },
      "outputs": [],
      "source": [
        "load_original_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peB3_ExnPQl0"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrsaL8Ew-ctL"
      },
      "outputs": [],
      "source": [
        "with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "  artifact = run.use_artifact('neuralearn/Malaria-Detection/new_dataset:v1', type='raw_data')\n",
        "  artifact_dir = artifact.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTgqIEHK-c1d"
      },
      "outputs": [],
      "source": [
        "IM_SIZE = 224\n",
        "def resize_rescale(image):\n",
        "  return tf.image.resize(image, (IM_SIZE, IM_SIZE))/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-2IapMA-c6c"
      },
      "outputs": [],
      "source": [
        "def preprocess_data():\n",
        "  with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "\n",
        "    artifact = run.use_artifact('neuralearn/Malaria-Detection/new_dataset:v1', type='raw_data')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    preprocessed_data = wandb.Artifact(\n",
        "        name = \"preprocessed_dataset\",\n",
        "        type=\"preprocessed_data\",\n",
        "        description = \"A Preprocessed version of the Malaria dataset\",\n",
        "\n",
        "    )\n",
        "\n",
        "    artifact_directory = \"artifacts/new_dataset:v1/\"\n",
        "\n",
        "    dataset_x = []\n",
        "    dataset_y = []\n",
        "\n",
        "    for f in os.listdir(artifact_directory)[:1000]:\n",
        "      with open(artifact_directory + f, 'rb') as file:\n",
        "        npz_array = np.load(file, allow_pickle = True)\n",
        "\n",
        "        x,y = npz_array.f.arr_0\n",
        "\n",
        "        dataset_x.append(resize_rescale(x))\n",
        "        dataset_y.append(y)\n",
        "\n",
        "    #dataset = tf.data.Dataset.from_tensor_slices((dataset_x, dataset_y))\n",
        "\n",
        "    with preprocessed_data.new_file(\"prep_dataset.npz\", mode = \"wb\") as file:\n",
        "        np.savez(file, [dataset_x, dataset_y])\n",
        "    run.log_artifact(preprocessed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-qBgAOMmFk8"
      },
      "outputs": [],
      "source": [
        "preprocess_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNqYhbE3pCzJ"
      },
      "source": [
        "### Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFTaqcEjuGkG"
      },
      "outputs": [],
      "source": [
        "run = wandb.init()\n",
        "artifact = run.use_artifact('neuralearn/Malaria-Detection/preprocessed_dataset:v2', type='preprocessed_data')\n",
        "artifact_dir = artifact.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBq_XWI5uGp3"
      },
      "outputs": [],
      "source": [
        "def split_data():\n",
        "  with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "\n",
        "    artifact = run.use_artifact('neuralearn/Malaria-Detection/preprocessed_dataset:v2', type='preprocessed_data')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    train_data = wandb.Artifact(\n",
        "        name = \"train_dataset\",\n",
        "        type=\"preprocessed_data\",\n",
        "        description = \"Training dataset\",\n",
        "\n",
        "    )\n",
        "    val_data = wandb.Artifact(\n",
        "\n",
        "        name = \"val_dataset\",\n",
        "        type=\"preprocessed_data\",\n",
        "        description = \"Validation dataset\",\n",
        "\n",
        "        )\n",
        "    test_data = wandb.Artifact(\n",
        "        name = \"test_dataset\",\n",
        "        type=\"preprocessed_data\",\n",
        "        description = \"Test dataset\",\n",
        "\n",
        "        )\n",
        "\n",
        "    artifact_file = \"artifacts/preprocessed_dataset:v2/prep_dataset.npz\"\n",
        "\n",
        "    with open(artifact_file, 'rb') as file:\n",
        "      npz_arr = np.load(file, allow_pickle = True)\n",
        "      arr = npz_arr.f.arr_0\n",
        "\n",
        "    train_split = 0.8\n",
        "    val_split = 0.1\n",
        "    test_split = 0.1\n",
        "\n",
        "    data_len = len(arr[0])\n",
        "\n",
        "    train_arr = [arr[0][0:int(train_split*data_len)], arr[1][0:int(train_split*data_len)]]\n",
        "    val_arr = [arr[0][int(train_split*data_len):int((train_split+val_split)*data_len)], arr[1][int(train_split*data_len):int((train_split+val_split)*data_len)] ]\n",
        "    test_arr = [arr[0][int((train_split+val_split)*data_len):], arr[1][int((train_split+val_split)*data_len):] ]\n",
        "\n",
        "\n",
        "    with train_data.new_file(\"train_dataset.npz\", mode = \"wb\") as file:\n",
        "        np.savez(file, train_arr)\n",
        "\n",
        "    with val_data.new_file(\"val_dataset.npz\", mode = \"wb\") as file:\n",
        "        np.savez(file, val_arr)\n",
        "\n",
        "    with test_data.new_file(\"test_dataset.npz\", mode = \"wb\") as file:\n",
        "        np.savez(file, test_arr)\n",
        "\n",
        "\n",
        "    run.log_artifact(train_data)\n",
        "    run.log_artifact(val_data)\n",
        "    run.log_artifact(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8JbsekwuT3X"
      },
      "outputs": [],
      "source": [
        "split_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYBajAUcpH9q"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlyK2BQZpKE4"
      },
      "outputs": [],
      "source": [
        "### tf.image augment\n",
        "def augment(image):\n",
        "  image = tf.image.rot90(image)\n",
        "  image = tf.image.flip_left_right(image)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llwsknTp7LYb"
      },
      "outputs": [],
      "source": [
        "/contertifant/acts/train_dataset:v0/train_dataset.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l45yjHt27vkD"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO5J39tfFeef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee5e2tQw7vmk"
      },
      "outputs": [],
      "source": [
        "def augment_data():\n",
        "  with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "\n",
        "    artifact = run.use_artifact('neuralearn/Malaria-Detection/train_dataset:v0', type='preprocessed_data')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    augmented_data = wandb.Artifact(\n",
        "        name = \"Augmented_dataset\",\n",
        "        type=\"preprocessed_data\",\n",
        "        description = \"An Augmented version of the Malaria train dataset\",\n",
        "    )\n",
        "\n",
        "    artifact_file = \"artifacts/train_dataset:v0/train_dataset.npz\"\n",
        "\n",
        "    dataset_x = []\n",
        "\n",
        "    with open(artifact_file, 'rb') as file:\n",
        "        npz_array = np.load(file, allow_pickle = True)\n",
        "\n",
        "        arr = npz_array.f.arr_0\n",
        "\n",
        "        for im in arr[0]:\n",
        "          dataset_x.append(augment(im))\n",
        "        dataset_y = arr[1]\n",
        "\n",
        "    with augmented_data.new_file(\"aug_dataset.npz\", mode = \"wb\") as file:\n",
        "        np.savez(file, [dataset_x, dataset_y])\n",
        "    run.log_artifact(augmented_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7t7hU_qh_Wgs"
      },
      "outputs": [],
      "source": [
        "augment_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUQgOMJGIZcU"
      },
      "source": [
        "# Model Creation and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYaYGqawya5T"
      },
      "source": [
        "## Wandb Model Versioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRS_ONJC_dy9"
      },
      "source": [
        "### Untrained Model Versioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH9h6T0XyfDi"
      },
      "outputs": [],
      "source": [
        "def log_model():\n",
        "  with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "\n",
        "\n",
        "    untrained_model = wandb.Artifact(\n",
        "        name = \"Untrained_model\",\n",
        "        type=\"model\",\n",
        "        description = \"The initial version of our lenet model\",\n",
        "        metadata = CONFIGURATION\n",
        "    )\n",
        "    filename = 'lenet.h5'\n",
        "    lenet_model.save(filename)\n",
        "\n",
        "    untrained_model.add_file(filename)\n",
        "    wandb.save(filename)\n",
        "    run.log_artifact(untrained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSGomNAD60oM"
      },
      "outputs": [],
      "source": [
        "log_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5HIIvUC_hsN"
      },
      "source": [
        "### Trained Model versioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpI1hw4i_jts"
      },
      "outputs": [],
      "source": [
        "def train_and_log():\n",
        "  with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "\n",
        "    artifact = run.use_artifact('neuralearn/Malaria-Detection/Augmented_dataset:v0', type='preprocessed_data')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    trained_sequential_model = wandb.Artifact(\n",
        "        name = \"Trained_Sequential_model\",\n",
        "        type=\"model\",\n",
        "        description = \"A trained version of our model\",\n",
        "        metadata = CONFIGURATION,\n",
        "    )\n",
        "\n",
        "    artifact_file = \"artifacts/Augmented_dataset:v0/aug_dataset.npz\"\n",
        "\n",
        "    dataset_x = []\n",
        "\n",
        "    with open(artifact_file, 'rb') as file:\n",
        "        npz_array = np.load(file, allow_pickle = True)\n",
        "\n",
        "        arr = npz_array.f.arr_0\n",
        "\n",
        "        for im in arr[0]:\n",
        "          dataset_x.append(im)\n",
        "        dataset_y = arr[1]\n",
        "\n",
        "\n",
        "    d_x = tf.convert_to_tensor(dataset_x, dtype = tf.float32)\n",
        "    d_y = tf.convert_to_tensor(dataset_y, dtype = tf.float32)\n",
        "\n",
        "    d = tf.data.Dataset.from_tensor_slices((d_x,d_y))\n",
        "\n",
        "    train_d = (\n",
        "        d\n",
        "        .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "\n",
        "    artifact = run.use_artifact('neuralearn/Malaria-Detection/Untrained_model:v0', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    artifact_file = \"artifacts/Untrained_model:v0/lenet.h5\"\n",
        "\n",
        "    lenet_model = tf.keras.models.load_model(artifact_file)\n",
        "\n",
        "    metrics = [TruePositives(name='tp'),FalsePositives(name='fp'), TrueNegatives(name='tn'), FalseNegatives(name='fn'),\n",
        "                BinaryAccuracy(name='accuracy'), Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
        "\n",
        "    lenet_model.compile(optimizer = Adam(learning_rate = CONFIGURATION['LEARNING_RATE']),\n",
        "          loss = BinaryCrossentropy(),\n",
        "          metrics = metrics)\n",
        "\n",
        "    lenet_model.fit(\n",
        "        train_d,\n",
        "        epochs = 3,\n",
        "        verbose = 1,\n",
        "        callbacks=[WandbCallback()],\n",
        "    )\n",
        "\n",
        "    filename = 'lenet_trained.h5'\n",
        "    lenet_model.save(filename)\n",
        "\n",
        "    trained_sequential_model.add_file(filename)\n",
        "    wandb.save(filename)\n",
        "    run.log_artifact(trained_sequential_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DywVLvth_jyL"
      },
      "outputs": [],
      "source": [
        "train_and_log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcZooLJ_fLsm"
      },
      "source": [
        "## Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QwVD7qdm9KK"
      },
      "outputs": [],
      "source": [
        "IM_SIZE = CONFIGURATION['IM_SIZE']\n",
        "DROPOUT_RATE = CONFIGURATION['DROPOUT_RATE']\n",
        "REGULARIZATION_RATE = CONFIGURATION['REGULARIZATION_RATE']\n",
        "N_FILTERS = CONFIGURATION['N_FILTERS']\n",
        "KERNEL_SIZE = CONFIGURATION['KERNEL_SIZE']\n",
        "POOL_SIZE = CONFIGURATION['POOL_SIZE']\n",
        "N_STRIDES = CONFIGURATION['N_STRIDES']\n",
        "\n",
        "lenet_model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n",
        "\n",
        "    Conv2D(filters = N_FILTERS , kernel_size = KERNEL_SIZE, strides = N_STRIDES , padding='valid',\n",
        "          activation = 'relu',kernel_regularizer = L2(REGULARIZATION_RATE)),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = POOL_SIZE, strides= N_STRIDES*2),\n",
        "    Dropout(rate = DROPOUT_RATE ),\n",
        "\n",
        "    Conv2D(filters = N_FILTERS*2 + 4, kernel_size = KERNEL_SIZE, strides=N_STRIDES, padding='valid',\n",
        "          activation = 'relu', kernel_regularizer = L2(REGULARIZATION_RATE)),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = POOL_SIZE, strides= N_STRIDES*2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense( CONFIGURATION['N_DENSE_1'], activation = \"relu\", kernel_regularizer = L2(REGULARIZATION_RATE)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate = DROPOUT_RATE),\n",
        "\n",
        "    Dense( CONFIGURATION['N_DENSE_2'], activation = \"relu\", kernel_regularizer = L2(REGULARIZATION_RATE)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(1, activation = \"sigmoid\"),\n",
        "\n",
        "])\n",
        "\n",
        "lenet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-snscgqO4u36"
      },
      "outputs": [],
      "source": [
        "def train_and_log():\n",
        "  with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "\n",
        "    artifact = run.use_artifact('neuralearn/Malaria-Detection/Augmented_dataset:v0', type='preprocessed_data')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    sequential_model = wandb.Artifact(\n",
        "        name = \"Sequential_model\",\n",
        "        type=\"model\",\n",
        "        description = \"A trained version of our model\",\n",
        "\n",
        "    )\n",
        "\n",
        "    artifact_file = \"artifacts/train_dataset:v0/aug_dataset.npz\"\n",
        "\n",
        "    dataset_x = []\n",
        "\n",
        "    with open(artifact_file, 'rb') as file:\n",
        "        npz_array = np.load(file, allow_pickle = True)\n",
        "\n",
        "        arr = npz_array.f.arr_0\n",
        "\n",
        "        for im in arr[0]:\n",
        "          dataset_x.append(augment(im))\n",
        "        dataset_y = arr[1]\n",
        "\n",
        "        # metrics = [TruePositives(name='tp'),FalsePositives(name='fp'), TrueNegatives(name='tn'), FalseNegatives(name='fn'),\n",
        "        #             BinaryAccuracy(name='accuracy'), Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
        "        # FACTOR = 1\n",
        "        # LABELS = ['Parasitized', 'Uninfected']\n",
        "\n",
        "\n",
        "        # lenet_model.compile(optimizer = Adam(learning_rate = CONFIGURATION['LEARNING_RATE']),\n",
        "        #       loss = BinaryCrossentropy(),\n",
        "        #       metrics = metrics)\n",
        "\n",
        "        # history = lenet_model.fit(\n",
        "        #     train_dataset,\n",
        "        #     validation_data = val_dataset,\n",
        "        #     epochs = 23,#CONFIGURATION['N_EPOCHS'],\n",
        "        #     verbose = 1,\n",
        "        #     #callbacks=[LogImagesCallbackWandB()]\n",
        "        #     )\n",
        "\n",
        "    # with preprocessed_data.new_file(\"aug_dataset.npz\", mode = \"wb\") as file:\n",
        "    #     np.savez(file, [dataset_x, dataset_y])\n",
        "\n",
        "    # run.log_artifact(preprocessed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc6sCUD84u6I"
      },
      "outputs": [],
      "source": [
        "train_and_log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAoTZNps4u8r"
      },
      "outputs": [],
      "source": [
        "artifact_file = \"artifacts/Augmented_dataset:v0/aug_dataset.npz\"\n",
        "\n",
        "dataset_x = []\n",
        "\n",
        "with open(artifact_file, 'rb') as file:\n",
        "    npz_array = np.load(file, allow_pickle = True)\n",
        "\n",
        "    arr = npz_array.f.arr_0\n",
        "\n",
        "    for im in arr[0]:\n",
        "      dataset_x.append(im)\n",
        "    dataset_y = arr[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRbHFk7m7fdi"
      },
      "outputs": [],
      "source": [
        "print(dataset_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vyPzaNb8xQF"
      },
      "outputs": [],
      "source": [
        "d_x = tf.convert_to_tensor(dataset_x, dtype = tf.float32)\n",
        "d_y = tf.convert_to_tensor(dataset_y, dtype = tf.float32)\n",
        "\n",
        "print(d_x.shape, d_y.shape)\n",
        "\n",
        "d = tf.data.Dataset.from_tensor_slices((d_x,d_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-hCCnHS_pOX"
      },
      "outputs": [],
      "source": [
        "for i in d.take(2):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWgnfnR2DRzO"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_d = (\n",
        "    d\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "print(train_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GbecTJ47uyr"
      },
      "outputs": [],
      "source": [
        "metrics = [TruePositives(name='tp'),FalsePositives(name='fp'), TrueNegatives(name='tn'), FalseNegatives(name='fn'),\n",
        "            BinaryAccuracy(name='accuracy'), Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
        "FACTOR = 1\n",
        "LABELS = ['Parasitized', 'Uninfected']\n",
        "\n",
        "\n",
        "lenet_model.compile(optimizer = Adam(learning_rate = CONFIGURATION['LEARNING_RATE']),\n",
        "      loss = BinaryCrossentropy(),\n",
        "      metrics = metrics)\n",
        "\n",
        "history = lenet_model.fit(\n",
        "    train_d,\n",
        "    #validation_data = val_dataset,\n",
        "    epochs = 2,#CONFIGURATION['N_EPOCHS'],\n",
        "    verbose = 1,\n",
        "    #callbacks=[LogImagesCallbackWandB()]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbm2WoMpfXod"
      },
      "source": [
        "## Functional API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMyO6e1C4u7"
      },
      "outputs": [],
      "source": [
        "func_input = Input(shape = (IM_SIZE, IM_SIZE, 3), name = \"Input Image\")\n",
        "\n",
        "x = Conv2D(filters = 6, kernel_size = 3, strides=1, padding='valid', activation = 'relu')(func_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D (pool_size = 2, strides= 2)(x)\n",
        "\n",
        "x = Conv2D(filters = 16, kernel_size = 3, strides=1, padding='valid', activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "output = MaxPool2D (pool_size = 2, strides= 2)(x)\n",
        "\n",
        "feature_extractor_model = Model(func_input, output, name = \"Feature_Extractor\")\n",
        "feature_extractor_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMYcqj4Kdnpt"
      },
      "outputs": [],
      "source": [
        "feature_extractor_seq_model = tf.keras.Sequential([\n",
        "                             InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n",
        "\n",
        "                             Conv2D(filters = 6, kernel_size = 3, strides=1, padding='valid', activation = 'relu'),\n",
        "                             BatchNormalization(),\n",
        "                             MaxPool2D (pool_size = 2, strides= 2),\n",
        "\n",
        "                             Conv2D(filters = 16, kernel_size = 3, strides=1, padding='valid', activation = 'relu'),\n",
        "                             BatchNormalization(),\n",
        "                             MaxPool2D (pool_size = 2, strides= 2),\n",
        "\n",
        "\n",
        "\n",
        "])\n",
        "feature_extractor_seq_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Wz41MHkzTu"
      },
      "source": [
        "## Callable Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrkk0npZfYBJ"
      },
      "outputs": [],
      "source": [
        "func_input = Input(shape = (IM_SIZE, IM_SIZE, 3), name = \"Input Image\")\n",
        "\n",
        "x = feature_extractor_seq_model(func_input)\n",
        "\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = Dense(100, activation = \"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Dense(10, activation = \"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "func_output = Dense(1, activation = \"sigmoid\")(x)\n",
        "\n",
        "lenet_model_func = Model(func_input, func_output, name = \"Lenet_Model\")\n",
        "lenet_model_func.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3arkQ973eIjM"
      },
      "source": [
        "## Model Subclassing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4JaTaX9lEPK"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(Layer):\n",
        "  def __init__(self, filters, kernel_size, strides, padding, activation, pool_size,):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "\n",
        "    self.conv_1 = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = padding, activation = activation)\n",
        "    self.batch_1 = BatchNormalization()\n",
        "    self.pool_1 = MaxPool2D (pool_size = pool_size, strides= 2*strides)\n",
        "\n",
        "    self.conv_2 = Conv2D(filters = filters*2, kernel_size = kernel_size, strides = strides, padding = padding, activation = activation)\n",
        "    self.batch_2 = BatchNormalization()\n",
        "    self.pool_2 = MaxPool2D (pool_size = pool_size, strides= 2*strides)\n",
        "\n",
        "  def call(self, x, training):\n",
        "\n",
        "    x = self.conv_1(x)\n",
        "    x = self.batch_1(x)\n",
        "    x = self.pool_1(x)\n",
        "\n",
        "    x = self.conv_2(x)\n",
        "    x = self.batch_2(x)\n",
        "    x = self.pool_2(x)\n",
        "\n",
        "    return x\n",
        "feature_sub_classed = FeatureExtractor(8, 3, 1, \"valid\", \"relu\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDPkCi9ZhQ70"
      },
      "outputs": [],
      "source": [
        "func_input = Input(shape = (IM_SIZE, IM_SIZE, 3), name = \"Input Image\")\n",
        "\n",
        "x = feature_sub_classed(func_input)\n",
        "\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = Dense(100, activation = \"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Dense(10, activation = \"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "func_output = Dense(1, activation = \"sigmoid\")(x)\n",
        "\n",
        "lenet_model_func = Model(func_input, func_output, name = \"Lenet_Model\")\n",
        "lenet_model_func.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8E1KU0QndJX"
      },
      "outputs": [],
      "source": [
        "class LenetModel(Model):\n",
        "  def __init__(self):\n",
        "    super(LenetModel, self).__init__()\n",
        "\n",
        "    self.feature_extractor = FeatureExtractor(8, 3, 1, \"valid\", \"relu\", 2)\n",
        "\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "    self.dense_1 = Dense(100, activation = \"relu\")\n",
        "    self.batch_1 = BatchNormalization()\n",
        "\n",
        "    self.dense_2 = Dense(10, activation = \"relu\")\n",
        "    self.batch_2 = BatchNormalization()\n",
        "\n",
        "    self.dense_3 = Dense(1, activation = \"sigmoid\")\n",
        "\n",
        "  def call(self, x, training):\n",
        "\n",
        "    x = self.feature_extractor(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense_1(x)\n",
        "    x = self.batch_1(x)\n",
        "    x = self.dense_2(x)\n",
        "    x = self.batch_2(x)\n",
        "    x = self.dense_3(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "lenet_sub_classed = LenetModel()\n",
        "lenet_sub_classed(tf.zeros([1,224,224,3]))\n",
        "lenet_sub_classed.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgF_fkT-qngT"
      },
      "source": [
        "## Custom Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKLHl1obqoVJ"
      },
      "outputs": [],
      "source": [
        "class NeuralearnDense(Layer):\n",
        "  def __init__(self, output_units, activation):\n",
        "    super(NeuralearnDense, self).__init__()\n",
        "    self.output_units = output_units\n",
        "    self.activation = activation\n",
        "\n",
        "  def build(self, input_features_shape):\n",
        "    self.w = self.add_weight(shape = (input_features_shape[-1], self.output_units), initializer = \"random_normal\", trainable = True)\n",
        "    self.b = self.add_weight(shape = (self.output_units,), initializer = \"random_normal\", trainable = True)\n",
        "\n",
        "  def call(self, input_features):\n",
        "\n",
        "    pre_output = tf.matmul(input_features, self.w) + self.b\n",
        "\n",
        "    if(self.activation == \"relu\"):\n",
        "      return tf.nn.relu(pre_output)\n",
        "\n",
        "    elif(self.activation == \"sigmoid\"):\n",
        "      return tf.math.sigmoid(pre_output)\n",
        "\n",
        "    else:\n",
        "      return pre_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz30O0yV2rGk"
      },
      "outputs": [],
      "source": [
        "IM_SIZE = 224\n",
        "lenet_custom_model = tf.keras.Sequential([\n",
        "                             InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n",
        "\n",
        "                             Conv2D(filters = 6, kernel_size = 3, strides=1, padding='valid', activation = 'relu'),\n",
        "                             BatchNormalization(),\n",
        "                             MaxPool2D (pool_size = 2, strides= 2),\n",
        "\n",
        "                             Conv2D(filters = 16, kernel_size = 3, strides=1, padding='valid', activation = 'relu'),\n",
        "                             BatchNormalization(),\n",
        "                             MaxPool2D (pool_size = 2, strides= 2),\n",
        "\n",
        "                             Flatten(),\n",
        "\n",
        "                             NeuralearnDense(100, activation = \"relu\"),\n",
        "                             BatchNormalization(),\n",
        "\n",
        "                             NeuralearnDense(10, activation = \"relu\"),\n",
        "                             BatchNormalization(),\n",
        "\n",
        "                             NeuralearnDense(1, activation = \"sigmoid\"),\n",
        "\n",
        "])\n",
        "lenet_custom_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ultY2FdfyX"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG6nVKSXdfcu"
      },
      "outputs": [],
      "source": [
        "class LossCallback(Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    print(\"\\n For Epoch Number {} the model has a loss of {} \".format(epoch+1, logs[\"loss\"]))\n",
        "\n",
        "  def on_batch_end(self, batch, logs):\n",
        "    print(\"\\n For Batch Number {} the model has a loss of {} \".format(batch+1, logs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9PSIylC4dAI"
      },
      "outputs": [],
      "source": [
        "test_dataset = test_dataset.batch(1)\n",
        "\n",
        "# images = wandb.Image(image_array, caption=\"Top: Output, Bottom: Input\")\n",
        "\n",
        "# wandb.log({\"examples\": images})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSzBOy-m4Fnh"
      },
      "outputs": [],
      "source": [
        "class LogImagesCallbackTensorBoard(Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    labels = []\n",
        "    inp = []\n",
        "\n",
        "    for x,y in test_dataset.as_numpy_iterator():\n",
        "      labels.append(y)\n",
        "      inp.append(x)\n",
        "    labels = np.array([i[0] for i in labels])\n",
        "    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n",
        "\n",
        "    threshold = 0.5\n",
        "\n",
        "    cm = confusion_matrix(labels, predicted > threshold)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "\n",
        "    sns.heatmap(cm, annot=True,)\n",
        "    plt.title('Confusion matrix - {}'.format(threshold))\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.axis('off')\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "    plt.savefig(buffer, format = 'png')\n",
        "\n",
        "    image = tf.image.decode_png(buffer.getvalue(), channels=3)\n",
        "    image = tf.expand_dims(image, axis = 0)\n",
        "\n",
        "    CURRENT_TIME = datetime.datetime.now().strftime('%d%m%y - %h%m%s')\n",
        "    IMAGE_DIR = './logs/' + CURRENT_TIME + '/images'\n",
        "    image_writer = tf.summary.create_file_writer(IMAGE_DIR)\n",
        "\n",
        "    with image_writer.as_default():\n",
        "      tf.summary.image(\"Training data\", image, step = epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTbdFZ93Tnzr"
      },
      "outputs": [],
      "source": [
        "class LogImagesCallbackWandBPlot(Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    labels = []\n",
        "    inp = []\n",
        "\n",
        "    for x,y in test_dataset.as_numpy_iterator():\n",
        "      labels.append(y)\n",
        "      inp.append(x)\n",
        "    labels = np.array([i[0] for i in labels])\n",
        "    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n",
        "\n",
        "    print(\"labels\", labels, labels.dtype)\n",
        "    print(\"predicted\", predicted, predicted.dtype)\n",
        "\n",
        "    pred = []\n",
        "\n",
        "    for i in range(len(predicted)):\n",
        "      if(predicted[i][0] < 0.5):\n",
        "        pred.append([1,0])\n",
        "      else:\n",
        "        pred.append([0,1])\n",
        "\n",
        "    pred = np.array(pred)\n",
        "\n",
        "    # wandb.log({\"Confusion Matrix\" : wandb.plot.confusion_matrix(\n",
        "    #     probs = pred,\n",
        "    #     y_true=labels,\n",
        "    #     class_names=[\"Parasitized\", \"Uninfected\"])})\n",
        "\n",
        "    wandb.log({\"ROC Curve\" : wandb.plot.roc_curve(\n",
        "        y_true = labels,\n",
        "        y_probas = pred,\n",
        "        labels = ['Parasitized', 'Uninfected'],\n",
        "    )})\n",
        "\n",
        "    wandb.log({'loss':logs['loss']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng6lfPud-i7v"
      },
      "outputs": [],
      "source": [
        "class LogImagesCallbackWandB(Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    labels = []\n",
        "    inp = []\n",
        "\n",
        "    for x,y in test_dataset.as_numpy_iterator():\n",
        "      labels.append(y)\n",
        "      inp.append(x)\n",
        "    labels = np.array([i[0] for i in labels])\n",
        "    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n",
        "\n",
        "    threshold = 0.5\n",
        "\n",
        "    cm = confusion_matrix(labels, predicted > threshold)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "\n",
        "    sns.heatmap(cm, annot=True,)\n",
        "    plt.title('Confusion matrix - {}'.format(threshold))\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.axis('off')\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "    plt.savefig(buffer, format = 'png')\n",
        "\n",
        "    image_array = tf.image.decode_png(buffer.getvalue(), channels=3)\n",
        "\n",
        "    images = wandb.Image(image_array, caption=\"Confusion Matrix for epoch: {}\".format(epoch))\n",
        "\n",
        "    wandb.log(\n",
        "        {\"Confusion Matrix\": images})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "413m-l0pttrt"
      },
      "source": [
        "### CSVLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZKjzj32tsHN"
      },
      "outputs": [],
      "source": [
        "csv_callback = CSVLogger(\n",
        "    'logs.csv', separator=',', append=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLaHf-B12OH8"
      },
      "source": [
        "### EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr-2Nzup2OcU"
      },
      "outputs": [],
      "source": [
        "es_callback = EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=2, verbose=1,\n",
        "    mode='auto', baseline=None, restore_best_weights=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P04zuKYv_hex"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKAAjue_KM8G"
      },
      "outputs": [],
      "source": [
        "pip install -U tensorboard_plugin_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhhaW037s-9V"
      },
      "outputs": [],
      "source": [
        "!rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15BqZlHGKtW5"
      },
      "outputs": [],
      "source": [
        "CURRENT_TIME = datetime.datetime.now().strftime('%d%m%y - %h%m%s')\n",
        "METRIC_DIR = './logs/' + CURRENT_TIME + '/metrics'\n",
        "train_writer = tf.summary.create_file_writer(METRIC_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV7CC2YI_ht3"
      },
      "outputs": [],
      "source": [
        "LOG_DIR = './logs/'+ CURRENT_TIME\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq = 1, profile_batch = '100,132')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvoXsM8zC6bD"
      },
      "source": [
        "### LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b3eU9yTC69J"
      },
      "outputs": [],
      "source": [
        "def scheduler(epoch, lr):\n",
        "\n",
        "  if epoch <= 1:\n",
        "    learning_rate = lr\n",
        "  else:\n",
        "    learning_rate = lr * tf.math.exp(-0.1)\n",
        "    learning_rate = learning_rate.numpy()\n",
        "\n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('Learning Rate', data = learning_rate, step = epoch)\n",
        "  return learning_rate\n",
        "scheduler_callback = LearningRateScheduler(scheduler, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEau-uvfRWA5"
      },
      "source": [
        "### ModelCheckpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dBXgkarRZT6"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    'weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_precision', verbose=0, save_best_only=True,\n",
        "    save_weights_only=True, mode='auto', save_freq='epoch',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t5zQwWckCS9"
      },
      "source": [
        "### ReduceLearningRateOnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGkKibvGkC6A"
      },
      "outputs": [],
      "source": [
        "plateau_callback = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy', factor=0.1, patience=5, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrm7NNzuDd5k"
      },
      "source": [
        "## Custom Metric Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhN8td6XDeLC"
      },
      "outputs": [],
      "source": [
        "class CustomAccuracy(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name = 'Custom_Accuracy', FACTOR = 1):\n",
        "    super(CustomAccuracy, self).__init__()\n",
        "    self.FACTOR = FACTOR\n",
        "    self.accuracy = self.add_weight(name = name, initializer = 'zeros')\n",
        "\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight = None):\n",
        "    output = binary_accuracy(tf.cast(y_true, dtype = tf.float32), y_pred)*self.FACTOR\n",
        "    self.accuracy.assign(tf.math.count_nonzero(output, dtype = tf.float32)/tf.cast(len(output), dtype = tf.float32))\n",
        "\n",
        "  def result(self):\n",
        "    return self.accuracy\n",
        "\n",
        "  def reset_states(self):\n",
        "    self.accuracy.assign(0.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AcriqBA2IU4"
      },
      "source": [
        "## Custom Metric Method (without parametres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JvNwoUw2IfR"
      },
      "outputs": [],
      "source": [
        "def custom_accuracy(y_true, y_pred):\n",
        "  print(binary_accuracy(y_true, y_pred))\n",
        "  return binary_accuracy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEezFeL_8tr"
      },
      "source": [
        "## Custom Metric Method (with Parametres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU1emg1I_9CB"
      },
      "outputs": [],
      "source": [
        "def custom_accuracy(FACTOR):\n",
        "  def metric(y_true, y_pred):\n",
        "    return binary_accuracy(y_true, y_pred)* FACTOR\n",
        "  return metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0mircHogrvl"
      },
      "source": [
        "## Custom Loss Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlzFZ7wzgyec"
      },
      "outputs": [],
      "source": [
        "class CustomBCE(tf.keras.losses.Loss):\n",
        "  def __init__(self, FACTOR):\n",
        "    super(CustomBCE, self).__init__()\n",
        "    self.FACTOR = FACTOR\n",
        "  def call(self, y_true, y_pred):\n",
        "    bce = BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred)* self.FACTOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhawCc8jf8JB"
      },
      "source": [
        "## Custom Loss Method (with parametres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcmUelUKUShm"
      },
      "outputs": [],
      "source": [
        "def custom_bce(FACTOR):\n",
        "  def loss(y_true, y_pred):\n",
        "    bce = BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred)* FACTOR\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3I7Zhyufjsf"
      },
      "source": [
        "## Custom Loss Method (without parametres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTNBj8yqfsq-"
      },
      "outputs": [],
      "source": [
        "def custom_bce(y_true, y_pred):\n",
        "  bce = BinaryCrossentropy()\n",
        "  return bce(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AicRPfDS79d"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV-cIEXu6aZo"
      },
      "outputs": [],
      "source": [
        "metrics = [TruePositives(name='tp'),FalsePositives(name='fp'), TrueNegatives(name='tn'), FalseNegatives(name='fn'),\n",
        "            BinaryAccuracy(name='accuracy'), Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
        "FACTOR = 1\n",
        "LABELS = ['Parasitized', 'Uninfected']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXa7OBPBm9O2"
      },
      "outputs": [],
      "source": [
        "lenet_model.compile(optimizer = Adam(learning_rate = CONFIGURATION['LEARNING_RATE']),\n",
        "      loss = BinaryCrossentropy(),\n",
        "      metrics = metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLWDksoJ0hU0"
      },
      "outputs": [],
      "source": [
        "history = lenet_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data = val_dataset,\n",
        "    epochs = 23,#CONFIGURATION['N_EPOCHS'],\n",
        "    verbose = 1,\n",
        "    #callbacks=[LogImagesCallbackWandB()]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDlEZcNtAO2J"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxMyYeM-DmlM"
      },
      "source": [
        "## Hyperparameter Tuning with WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU9usxE4UcEo"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "  \"name\" : \"Malaria-Prediction-Sweep\",\n",
        "  \"method\" : \"random\",\n",
        "  \"metric\": {\n",
        "      \"name\" : \"accuracy\",\n",
        "      \"goal\" : \"maximize\",\n",
        "  },\n",
        "  \"parameters\" : {\n",
        "\n",
        "    \"IM_SIZE\": {\n",
        "        \"value\" : 224,\n",
        "    },\n",
        "\n",
        "    \"N_EPOCHS\": {\n",
        "        \"value\" : 1,\n",
        "    },\n",
        "\n",
        "    \"KERNEL_SIZE\": {\n",
        "        \"value\" : 3,\n",
        "    },\n",
        "\n",
        "    \"N_STRIDES\": {\n",
        "        \"value\" : 1,\n",
        "    },\n",
        "\n",
        "    \"POOL_SIZE\": {\n",
        "        \"value\" : 224,\n",
        "    },\n",
        "\n",
        "    \"N_FILTERS\" : {\n",
        "        \"value\" : 6,\n",
        "    },\n",
        "\n",
        "    \"N_DENSE_1\" : {\n",
        "      \"values\" : [16, 32, 64, 128]\n",
        "    },\n",
        "\n",
        "    \"N_DENSE_2\" : {\n",
        "      \"values\" : [16, 32, 64, 128]\n",
        "    },\n",
        "\n",
        "    \"DROPOUT_RATE\":{\n",
        "      \"min\": 0.1,\n",
        "      \"max\": 0.4\n",
        "    },\n",
        "\n",
        "    \"REGULARIZATION_RATE\" :{\n",
        "      \"distribution\": \"uniform\",\n",
        "      \"min\": 0.001,\n",
        "      \"max\": 0.1\n",
        "    },\n",
        "\n",
        "    \"LEARNING_RATE\" :{\n",
        "      \"distribution\": \"uniform\",\n",
        "      \"min\": 1e-4,\n",
        "      \"max\": 1e-2\n",
        "    }\n",
        "  },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDI4jVQkMZpU"
      },
      "outputs": [],
      "source": [
        "IM_SIZE = 224\n",
        "def model_tune(config):\n",
        "  lenet_model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape = (224, 224, 3)),\n",
        "\n",
        "    Conv2D(filters = 6 , kernel_size = 3, strides = 1 , padding='valid',\n",
        "          activation = 'relu',kernel_regularizer = L2(config['REGULARIZATION_RATE'])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = 1, strides= config['N_STRIDES']*2),\n",
        "    Dropout(rate = config['DROPOUT_RATE'] ),\n",
        "\n",
        "    Conv2D(filters = 16, kernel_size = 3, strides = 1, padding='valid',\n",
        "          activation = 'relu', kernel_regularizer = L2(config['REGULARIZATION_RATE'])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = 1, strides= 2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense( config['N_DENSE_1'], activation = \"relu\", kernel_regularizer = L2(config['REGULARIZATION_RATE'])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate = DROPOUT_RATE),\n",
        "\n",
        "    Dense( config['N_DENSE_2'], activation = \"relu\", kernel_regularizer = L2(config['REGULARIZATION_RATE'])),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(1, activation = \"sigmoid\"),\n",
        "\n",
        "  ])\n",
        "\n",
        "\n",
        "  return lenet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayj_1UaJM9Nk"
      },
      "outputs": [],
      "source": [
        "wandb.config = {\n",
        "  \"LEARNING_RATE\": 0.001,\n",
        "  \"N_EPOCHS\": 1,\n",
        "  \"BATCH_SIZE\": 128,\n",
        "  \"DROPOUT_RATE\": 0.0,\n",
        "  \"IM_SIZE\": 224,\n",
        "  \"REGULARIZATION_RATE\": 0.0,\n",
        "  \"N_FILTERS\": 6,\n",
        "  \"KERNEL_SIZE\": 3,\n",
        "  \"N_STRIDES\": 1,\n",
        "  \"POOL_SIZE\": 2,\n",
        "  \"N_DENSE_1\": 100,\n",
        "  \"N_DENSE_2\": 10,\n",
        "}\n",
        "CONFIGURATION = wandb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xb32xrsDsOA"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    with wandb.init(project=\"Malaria-Detection\", entity=\"neuralearn\") as run:\n",
        "        config = wandb.config\n",
        "        model = model_tune(config)\n",
        "        model.compile(\n",
        "              optimizer= Adam(\n",
        "                  learning_rate = config['LEARNING_RATE']),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        "              )\n",
        "        model.fit(val_dataset, epochs=2, callbacks = [WandbCallback()])\n",
        "        #wandb.log({\"loss\": loss, \"epoch\": epoch})\n",
        "\n",
        "count = 5 # number of runs to execute\n",
        "wandb.agent(sweep_id, function=train, count=count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzS9pcnOPRva"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAUfFaBvPT7c"
      },
      "outputs": [],
      "source": [
        "IM_SIZE = 224\n",
        "def model_tune(hparams):\n",
        "  lenet_model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n",
        "\n",
        "    Conv2D(filters = 6, kernel_size = 3, strides=1, padding='valid',\n",
        "          activation = 'relu',kernel_regularizer = L2(hparams[HP_REGULARIZATION_RATE])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = 2, strides= 2),\n",
        "    Dropout(rate = hparams[HP_DROPOUT]),\n",
        "\n",
        "    Conv2D(filters = 16, kernel_size = 3, strides=1, padding='valid',\n",
        "          activation = 'relu', kernel_regularizer = L2(hparams[HP_REGULARIZATION_RATE])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = 2, strides= 2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense( hparams[HP_NUM_UNITS_1], activation = \"relu\", kernel_regularizer = L2(hparams[HP_REGULARIZATION_RATE])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate = hparams[HP_DROPOUT]),\n",
        "\n",
        "    Dense(hparams[HP_NUM_UNITS_2], activation = \"relu\", kernel_regularizer = L2(hparams[HP_REGULARIZATION_RATE])),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(1, activation = \"sigmoid\"),\n",
        "  ])\n",
        "\n",
        "  lenet_model.compile(\n",
        "        optimizer= Adam(learning_rate = hparams[HP_LEARNING_RATE]),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "  lenet_model.fit(val_dataset, epochs=1)\n",
        "  _, accuracy = lenet_model.evaluate(val_dataset)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG9GX1iNdAfB"
      },
      "outputs": [],
      "source": [
        "HP_NUM_UNITS_1 = hp.HParam('num_units_1', hp.Discrete([16,32,64,128]))\n",
        "HP_NUM_UNITS_2 = hp.HParam('num_units_2', hp.Discrete([16,32,64,128]))\n",
        "HP_DROPOUT = hp.HParam('dropout_rate', hp.Discrete([0.1,0.2,0.3]))\n",
        "HP_REGULARIZATION_RATE = hp.HParam('regularization_rate', hp.Discrete([0.001,0.01,0.1]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-4, 1e-3]))\n",
        "\n",
        "fixed range of values is very large\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH58PrBWuE68"
      },
      "outputs": [],
      "source": [
        "run_number = 0\n",
        "for num_units_1 in HP_NUM_UNITS_1.domain.values:\n",
        "  for num_units_2 in HP_NUM_UNITS_2.domain.values:\n",
        "    for dropout_rate in HP_DROPOUT.domain.values:\n",
        "      for regularization_rate in HP_REGULARIZATION_RATE.domain.values:\n",
        "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
        "\n",
        "          hparams = {\n",
        "              HP_NUM_UNITS_1: num_units_1,\n",
        "              HP_NUM_UNITS_2: num_units_2,\n",
        "              HP_DROPOUT: dropout_rate,\n",
        "              HP_REGULARIZATION_RATE: regularization_rate,\n",
        "              HP_LEARNING_RATE: learning_rate,\n",
        "\n",
        "          }\n",
        "          file_writer = tf.summary.create_file_writer('logs/hparams-' + str(run_number))\n",
        "\n",
        "          with file_writer.as_default():\n",
        "              hp.hparams(hparams)\n",
        "              accuracy = model_tune(hparams)\n",
        "              tf.summary.scalar('accuracy', accuracy, step = 0)\n",
        "          print(\"For the run {}, hparams num_units_1:{}, num_units_2:{}, dropout:{}, regularization_rate:{}, learning_rate:{}\".format(run_number, hparams[HP_NUM_UNITS_1], hparams[HP_NUM_UNITS_2],\n",
        "                                                             hparams[HP_DROPOUT], hparams[HP_REGULARIZATION_RATE],\n",
        "                                                             hparams[HP_LEARNING_RATE]))\n",
        "          run_number += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQBpIvpDSyAD"
      },
      "source": [
        "## Custom Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fgzFUyY_Jh0"
      },
      "outputs": [],
      "source": [
        "OPTIMIZER = Adam(learning_rate = 0.01)\n",
        "METRIC = BinaryAccuracy()\n",
        "METRIC_VAL = BinaryAccuracy()\n",
        "EPOCHS = CONFIGURATION['N_EPOCHS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO_TeQKoqBkf"
      },
      "outputs": [],
      "source": [
        "CURRENT_TIME = datetime.datetime.now().strftime('%d%m%y - %h%m%s')\n",
        "CUSTOM_TRAIN_DIR = './logs/' + CURRENT_TIME + '/custom/train'\n",
        "CUSTOM_VAL_DIR = './logs/' + CURRENT_TIME + '/custom/val'\n",
        "\n",
        "custom_train_writer = tf.summary.create_file_writer(CUSTOM_TRAIN_DIR)\n",
        "custom_val_writer = tf.summary.create_file_writer(CUSTOM_VAL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3toQq0HNQofo"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def training_block(x_batch, y_batch):\n",
        "  with tf.GradientTape() as recorder:\n",
        "      y_pred = lenet_model(x_batch, training = True)\n",
        "      loss = custom_bce(y_batch, y_pred)\n",
        "\n",
        "  #wandb.log({'loss':loss.numpy()})\n",
        "  partial_derivatives = recorder.gradient(loss, lenet_model.trainable_weights)\n",
        "  OPTIMIZER.apply_gradients(zip(partial_derivatives, lenet_model.trainable_weights))\n",
        "  METRIC.update_state(y_batch, y_pred)\n",
        "  return loss\n",
        "\n",
        "@tf.function\n",
        "def val_block(x_batch_val, y_batch_val):\n",
        "    y_pred_val = lenet_model(x_batch_val, training = False)\n",
        "    loss_val = custom_bce(y_batch_val, y_pred_val)\n",
        "    METRIC_VAL.update_state(y_batch_val, y_pred_val)\n",
        "    return loss_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8UMV43MXZyF"
      },
      "outputs": [],
      "source": [
        "def neuralearn(model, loss_function, METRIC, VAL_METRIC, OPTIMIZER, train_dataset, val_dataset, EPOCHS):\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(\"Training starts for epoch number {}\".format(epoch+1))\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "      loss = training_block(x_batch, y_batch)\n",
        "\n",
        "    print(\"Training Loss\", loss)\n",
        "    print(\"The accuracy is: \", METRIC.result())\n",
        "\n",
        "    with custom_train_writer.as_default():\n",
        "      tf.summary.scalar('Training Loss', data = loss, step = epoch)\n",
        "    with custom_train_writer.as_default():\n",
        "      tf.summary.scalar('Training Accuracy', data = METRIC.result(), step = epoch)\n",
        "\n",
        "    METRIC.reset_states()\n",
        "\n",
        "    for (x_batch_val, y_batch_val) in val_dataset:\n",
        "      loss_val = val_block(x_batch_val, y_batch_val)\n",
        "\n",
        "    print(\"The Validation loss\", loss_val)\n",
        "    print(\"The Validation accuracy is: \", METRIC_VAL.result())\n",
        "\n",
        "    with custom_val_writer.as_default():\n",
        "      tf.summary.scalar('Validation Loss', data = loss_val, step = epoch)\n",
        "    with custom_val_writer.as_default():\n",
        "      tf.summary.scalar('Validation Accuracy', data = METRIC_VAL.result(), step = epoch)\n",
        "\n",
        "    METRIC_VAL.reset_states()\n",
        "  print(\"Training Complete!!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkAe8ZV0YO9d"
      },
      "outputs": [],
      "source": [
        "neuralearn(lenet_model, custom_bce, METRIC, METRIC_VAL, OPTIMIZER, train_dataset, val_dataset, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqTbzoj2266w"
      },
      "outputs": [],
      "source": [
        "# image = cv2.imread('cell.jpg')\n",
        "# print(image.shape)\n",
        "# image = tf.expand_dims(image, axis = 0)\n",
        "# print(image.shape)\n",
        "\n",
        "# lenet_model.predict(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdq-5nzgBgC1"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nURZ0KN8SAAK"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmgGVhNwJJ4F"
      },
      "outputs": [],
      "source": [
        "tensorboard --logdir=logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKt3lx4N0hfv"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss', 'val_loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4FAk3gY0hg8"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train_accuracy', 'val_accuracy'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h_6C48b86CN"
      },
      "source": [
        "# **Model Evaluation and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRGfTlSA-5Gp"
      },
      "outputs": [],
      "source": [
        "test_dataset = test_dataset.batch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1lUnFwwm9bc"
      },
      "outputs": [],
      "source": [
        "lenet_model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyUnLjaOJRZN"
      },
      "source": [
        "## Visualizing Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAbnEi-StSqV"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "inp = []\n",
        "# for t in test_dataset:\n",
        "#   print(t)\n",
        "#   break\n",
        "for x,y in test_dataset.as_numpy_iterator():\n",
        "  labels.append(y)\n",
        "  inp.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzk8ziZM06PP"
      },
      "outputs": [],
      "source": [
        "print(np.array(inp).shape)\n",
        "print(np.array(inp)[:,0,...].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zz5CtEZLigm"
      },
      "outputs": [],
      "source": [
        "labels = np.array([i[0] for i in labels])\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wk_Vr8iOQN7"
      },
      "outputs": [],
      "source": [
        "predicted = lenet_model.predict(np.array(inp)[:,0,...])\n",
        "print(predicted[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdBB3_-RO2nf"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "\n",
        "cm = confusion_matrix(labels, predicted > threshold)\n",
        "print(cm)\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "sns.heatmap(cm, annot=True,)\n",
        "plt.title('Confusion matrix - {}'.format(threshold))\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFbX78RpN3iC"
      },
      "outputs": [],
      "source": [
        "#tp: 1267.0000 - fp: 99.0000 - tn: 1298.0000 - fn: 93.0000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6iDg1tvHFkz"
      },
      "source": [
        "## ROC Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZUiZnMGN-7-"
      },
      "outputs": [],
      "source": [
        "fp, tp, thresholds = roc_curve(labels, predicted)\n",
        "plt.plot(fp, tp)\n",
        "plt.xlabel(\"False Positive rate\")\n",
        "plt.ylabel(\"True Positive rate\")\n",
        "\n",
        "plt.grid()\n",
        "\n",
        "skip = 20\n",
        "\n",
        "for i in range(0, len(thresholds), skip):\n",
        "  plt.text(fp[i], tp[i], thresholds[i])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BNXfkonHEeT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5Phk0rRm9dy"
      },
      "outputs": [],
      "source": [
        "parasite_or_not(lenet_model.predict(test_dataset.take(1))[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aFmzOJCOnOt"
      },
      "outputs": [],
      "source": [
        "def parasite_or_not(x):\n",
        "  if(x<0.5):\n",
        "    return str('P')\n",
        "  else:\n",
        "    return str('U')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9vuhe3wm9gk"
      },
      "outputs": [],
      "source": [
        "for i, (image, label) in enumerate(test_dataset.take(9)):\n",
        "\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(image[0])\n",
        "  plt.title(str(parasite_or_not(label.numpy()[0])) + \":\" + str(parasite_or_not(lenet_loaded_model.predict(image)[0][0])))\n",
        "\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXKMU54Im9jv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvqGMT1Xvtv"
      },
      "source": [
        "# Loading and Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkeTbJxZm9mS"
      },
      "outputs": [],
      "source": [
        "lenet_model.save(\"lenet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yykivbhcm9o-"
      },
      "outputs": [],
      "source": [
        "lenet_loaded_model = tf.keras.models.load_model(\"lenets\")\n",
        "lenet_loaded_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vKaCNEYm9rW"
      },
      "outputs": [],
      "source": [
        "lenet_model.save(\"lenet.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlCI1ibKm9t4"
      },
      "outputs": [],
      "source": [
        "lenet_loaded_model = tf.keras.models.load_model(\"lenet.hdf5\")\n",
        "lenet_loaded_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVam3ZQ2m9wQ"
      },
      "outputs": [],
      "source": [
        "lenet_model.save_weights(\"weights/lenet_weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff3EohH2m9yi"
      },
      "outputs": [],
      "source": [
        "lenet_weights_model = lenet_model.load_weights(\"weights/lenet_weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cr7sRFKtYvO"
      },
      "source": [
        "## Saving to and Loading from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVJGZOsMecr_"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fgJf4GHecu8"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/lenet/ /content/drive/MyDrive/lenet_colab/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5qR6IMXecxa"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/lenet_colab/ /content/lenet_colab/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJELckJC02YI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPcnmqoJcL7W"
      },
      "outputs": [],
      "source": [
        "image_1 = cv2.resize(cv2.imread('car.jpg'), (2560, 1440))/255\n",
        "image_2 = cv2.resize(cv2.imread('train.jpg'), (2560, 1440))/255\n",
        "print(image_1.shape, image_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfTgp71H4uwj"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(100, 100), dpi=80)\n",
        "\n",
        "lamda = 0.6\n",
        "image = lamda*image_1 + (1-lamda)*image_2\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.savefig('image.jpg')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "DK7_p02_b6_m",
        "-1QWh3KjbsuJ",
        "3BUZMRuhV-HL",
        "z_RZU3vxG2Aj",
        "im-Eo6tvHgmM",
        "IylKvmx5LUCR",
        "PTW0fKw313gt",
        "TeVM_g7JsaQr",
        "AsB9YnCuPITj",
        "peB3_ExnPQl0",
        "hNqYhbE3pCzJ",
        "kbm2WoMpfXod",
        "11Wz41MHkzTu",
        "3arkQ973eIjM",
        "xgF_fkT-qngT",
        "20ultY2FdfyX",
        "413m-l0pttrt",
        "lLaHf-B12OH8",
        "P04zuKYv_hex",
        "yvoXsM8zC6bD",
        "ZEau-uvfRWA5",
        "-t5zQwWckCS9",
        "Rrm7NNzuDd5k",
        "_AcriqBA2IU4",
        "2bEezFeL_8tr",
        "-0mircHogrvl",
        "JhawCc8jf8JB",
        "J3I7Zhyufjsf",
        "DxMyYeM-DmlM",
        "zzS9pcnOPRva",
        "HQBpIvpDSyAD",
        "qdq-5nzgBgC1",
        "1h_6C48b86CN",
        "dyUnLjaOJRZN",
        "X6iDg1tvHFkz",
        "utvqGMT1Xvtv",
        "0Cr7sRFKtYvO"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}